{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import *\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/dkmoon/Desktop/WorkSpace/DKU/t-distribution MeNet/Code/t-menet/t_menet\n"
     ]
    }
   ],
   "source": [
    "# Data Load\n",
    "print(os.getcwd())\n",
    "data_path = \"./data/t_distribution_data.csv\"\n",
    "data = pd.read_csv(data_path)\n",
    "\n",
    "# Data Preprocessing\n",
    "le = LabelEncoder()\n",
    "data[\"id\"] = le.fit_transform(data[\"id\"])\n",
    "clusters = sorted(list(set(data[\"id\"].values)))  # unique cluster id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Torch Dataset Class\n",
    "class SIMDataset(Dataset):\n",
    "    def __init__(self, data, targets, clusters):\n",
    "        self.data = data\n",
    "        self.targets = targets\n",
    "        self.clusters = clusters\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return self.data[index], self.targets[index], self.clusters[index]\n",
    "\n",
    "\n",
    "x_col = [\"id\", \"x\"]  # input columns\n",
    "y_col = [\"y\"]  # target columns\n",
    "\n",
    "# Train-Test Split\n",
    "SEED = 42\n",
    "train, test = train_test_split(\n",
    "    data, test_size=0.333, random_state=SEED, stratify=data[\"id\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset Creation\n",
    "train_dataset = SIMDataset(train[x_col].values, train[y_col].values, train[\"id\"].values)\n",
    "test_dataset = SIMDataset(test[x_col].values, test[y_col].values, test[\"id\"].values)\n",
    "\n",
    "batch_size = 32\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define PyTorch MeNet Model\n",
    "class MeNet(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim=[20, 5]):\n",
    "        super(MeNet, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_dim, hidden_dim[0])\n",
    "        self.fc2 = nn.Linear(hidden_dim[0], hidden_dim[1])\n",
    "        self.fc3 = nn.Linear(hidden_dim[1], 1)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.relu(self.fc1(x))\n",
    "        x = self.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# t-분포 손실 함수 정의\n",
    "def t_loss(k):\n",
    "    def loss(y_true, y_pred):\n",
    "        error = y_true - y_pred\n",
    "        squared_error = torch.square(error)\n",
    "        scaled_error = torch.log(k + squared_error)\n",
    "        return torch.mean(scaled_error)\n",
    "\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model Training\n",
    "def train_menet(\n",
    "    model, train_loader, n_clusters, device, k=1.0, epochs=100, lr=0.001, patience=10\n",
    "):\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "    # criterion = nn.MSELoss()\n",
    "    criterion = t_loss(k)  # Apply t-distribution loss\n",
    "\n",
    "    # Initialize Random Effects\n",
    "    output_dim = model.fc2.weight.data.size(0)  # Output dimension from fc2 weights\n",
    "    b_hat = torch.zeros(n_clusters, output_dim, device=device)\n",
    "    D_hat = torch.eye(output_dim, device=device)\n",
    "    sig2e_est = 1.0\n",
    "\n",
    "    best_loss = float(\"inf\")\n",
    "    wait = 0\n",
    "\n",
    "    for epoch in tqdm(range(epochs)):\n",
    "        model.train()\n",
    "        epoch_loss = 0\n",
    "        for batch in train_loader:\n",
    "            X_batch, y_batch, cluster_ids = batch\n",
    "            X_batch, y_batch = X_batch.to(device).float(), y_batch.to(device).float()\n",
    "\n",
    "            # Forward pass to get Z (feature map)\n",
    "            with torch.no_grad():\n",
    "                feature_map = model.fc2(\n",
    "                    model.relu(model.fc1(X_batch))\n",
    "                )  # Shape: [batch_size, hidden_dim[1]]\n",
    "\n",
    "            # Debugging Z shape\n",
    "            # print(f\"Feature Map shape: {feature_map.shape}, Expected shape: [batch_size, {output_dim}]\")\n",
    "\n",
    "            # E-Step: Update Random Effects\n",
    "            for cluster_id in range(n_clusters):\n",
    "                indices = (cluster_ids == cluster_id).nonzero(as_tuple=True)[0]\n",
    "                if indices.numel() == 0:\n",
    "                    continue\n",
    "\n",
    "                Z_i = feature_map[indices]  # Feature map for the current cluster\n",
    "                y_i = y_batch[indices].squeeze(\n",
    "                    -1\n",
    "                )  # Remove extra dimension, Shape: [len(indices)]\n",
    "                f_hat_i = model(X_batch[indices]).squeeze()  # Shape: [len(indices)]\n",
    "\n",
    "                # Debugging shapes\n",
    "                # print(f\"Z_i shape: {Z_i.shape}, D_hat shape: {D_hat.shape}, y_i shape: {y_i.shape}, f_hat_i shape: {f_hat_i.shape}\")\n",
    "\n",
    "                # Ensure dimensions match for matrix multiplication\n",
    "                V_hat_i = Z_i @ D_hat @ Z_i.T + sig2e_est * torch.eye(\n",
    "                    Z_i.size(0), device=device\n",
    "                )\n",
    "                V_hat_inv_i = torch.linalg.inv(V_hat_i)\n",
    "\n",
    "                # Expand (y_i - f_hat_i) to 2D for matrix multiplication\n",
    "                residual = (y_i - f_hat_i).unsqueeze(-1)  # Shape: [len(indices), 1]\n",
    "\n",
    "                # Update b_hat\n",
    "                b_hat[cluster_id] = (D_hat @ Z_i.T @ V_hat_inv_i @ residual).squeeze(-1)\n",
    "\n",
    "            # M-Step: Update Fixed Effects\n",
    "            optimizer.zero_grad()\n",
    "            y_pred = model(X_batch).squeeze()\n",
    "            loss = criterion(y_pred, y_batch)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            epoch_loss += loss.item()\n",
    "\n",
    "        epoch_loss /= len(train_loader)\n",
    "        print(f\"Epoch {epoch + 1}/{epochs} : Loss {epoch_loss:.4f}\")\n",
    "\n",
    "        # Early Stopping\n",
    "        if epoch_loss < best_loss:\n",
    "            best_loss = epoch_loss\n",
    "            wait = 0\n",
    "        else:\n",
    "            wait += 1\n",
    "            if wait >= patience:\n",
    "                print(\"Early stopping triggered\")\n",
    "                break\n",
    "\n",
    "    return model, b_hat, sig2e_est"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model Inference\n",
    "def menet_predict(model, test_loader, b_hat, n_clusters, device):\n",
    "    \"\"\"\n",
    "    Perform inference using the trained MeNet model with random effects.\n",
    "\n",
    "    Args:\n",
    "        model: Trained PyTorch model.\n",
    "        test_loader: DataLoader for the test dataset.\n",
    "        b_hat: Tensor containing random effect estimates for each cluster.\n",
    "        n_clusters: Number of clusters (unique IDs).\n",
    "        device: Device for computation (CPU or GPU).\n",
    "\n",
    "    Returns:\n",
    "        predictions: NumPy array of predictions for the test dataset.\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    predictions = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in test_loader:\n",
    "            X_batch, _, cluster_ids = batch\n",
    "            X_batch = X_batch.to(device).float()\n",
    "\n",
    "            # Forward pass through the model to compute fixed effects\n",
    "            y_pred = model(X_batch).squeeze()  # Shape: [batch_size]\n",
    "\n",
    "            # Apply random effects to each cluster\n",
    "            for cluster_id in range(n_clusters):\n",
    "                indices = (cluster_ids == cluster_id).nonzero(as_tuple=True)[0]\n",
    "                if indices.numel() == 0:\n",
    "                    continue\n",
    "\n",
    "                # Apply random effects adjustment for the current cluster\n",
    "                feature_map = model.fc2(\n",
    "                    model.relu(model.fc1(X_batch))\n",
    "                )  # Shape: [batch_size, hidden_dim[1]]\n",
    "                Z_i = feature_map[indices]  # Feature map for the current cluster\n",
    "                adjustment = Z_i @ b_hat[cluster_id].unsqueeze(\n",
    "                    -1\n",
    "                )  # Shape: [len(indices), 1]\n",
    "                y_pred[indices] += adjustment.squeeze(-1)\n",
    "\n",
    "            predictions.extend(y_pred.cpu().numpy())  # Convert predictions to NumPy\n",
    "\n",
    "    return np.array(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Main Execution\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "input_dim = len(x_col)\n",
    "n_clusters = len(clusters)\n",
    "k = 7.0  # Degrees of freedom for t-distribution loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 11%|█         | 11/100 [00:00<00:01, 47.62it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100 : Loss 1.9506\n",
      "Epoch 2/100 : Loss 1.9494\n",
      "Epoch 3/100 : Loss 1.9490\n",
      "Epoch 4/100 : Loss 1.9486\n",
      "Epoch 5/100 : Loss 1.9483\n",
      "Epoch 6/100 : Loss 1.9480\n",
      "Epoch 7/100 : Loss 1.9479\n",
      "Epoch 8/100 : Loss 1.9477\n",
      "Epoch 9/100 : Loss 1.9477\n",
      "Epoch 10/100 : Loss 1.9476\n",
      "Epoch 11/100 : Loss 1.9475\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 16%|█▌        | 16/100 [00:00<00:01, 43.71it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12/100 : Loss 1.9475\n",
      "Epoch 13/100 : Loss 1.9474\n",
      "Epoch 14/100 : Loss 1.9474\n",
      "Epoch 15/100 : Loss 1.9474\n",
      "Epoch 16/100 : Loss 1.9474\n",
      "Epoch 17/100 : Loss 1.9474\n",
      "Epoch 18/100 : Loss 1.9474\n",
      "Epoch 19/100 : Loss 1.9474\n",
      "Epoch 20/100 : Loss 1.9473\n",
      "Epoch 21/100 : Loss 1.9473\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 28%|██▊       | 28/100 [00:00<00:01, 48.92it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 22/100 : Loss 1.9473\n",
      "Epoch 23/100 : Loss 1.9473\n",
      "Epoch 24/100 : Loss 1.9474\n",
      "Epoch 25/100 : Loss 1.9474\n",
      "Epoch 26/100 : Loss 1.9473\n",
      "Epoch 27/100 : Loss 1.9473\n",
      "Epoch 28/100 : Loss 1.9473\n",
      "Epoch 29/100 : Loss 1.9473\n",
      "Epoch 30/100 : Loss 1.9473\n",
      "Epoch 31/100 : Loss 1.9473\n",
      "Epoch 32/100 : Loss 1.9473\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 36%|███▌      | 36/100 [00:00<00:01, 47.25it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 33/100 : Loss 1.9473\n",
      "Epoch 34/100 : Loss 1.9473\n",
      "Epoch 35/100 : Loss 1.9473\n",
      "Epoch 36/100 : Loss 1.9473\n",
      "Epoch 37/100 : Loss 1.9473\n",
      "Early stopping triggered\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Model Initialization\n",
    "model = MeNet(input_dim).to(device)\n",
    "\n",
    "# Train the Model\n",
    "model, b_hat, sig2e_est = train_menet(\n",
    "    model, train_loader, n_clusters, device, k=k, epochs=100\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict\n",
    "predictions = menet_predict(\n",
    "    model=model,\n",
    "    test_loader=test_loader,\n",
    "    b_hat=b_hat,\n",
    "    n_clusters=n_clusters,\n",
    "    device=device,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===== Evaluation Metrics =====\n",
      "MSE: 0.0200\n",
      "MAE: 0.0902\n",
      "MAPE: 103552.1810\n",
      "MRPE: 216.8140\n",
      "t_loss: 1.9487\n"
     ]
    }
   ],
   "source": [
    "# t-분포 손실 함수 정의\n",
    "def t_loss_metric(y_true, y_pred, k=1.0):\n",
    "    \"\"\"\n",
    "    Compute the t-distribution loss as a metric for evaluation.\n",
    "\n",
    "    Args:\n",
    "        y_true (np.array): Ground truth values.\n",
    "        y_pred (np.array): Predicted values.\n",
    "        k (float): Scaling parameter (degrees of freedom).\n",
    "\n",
    "    Returns:\n",
    "        float: t-distribution loss value.\n",
    "    \"\"\"\n",
    "    error = y_true - y_pred\n",
    "    squared_error = np.square(error)\n",
    "    scaled_error = np.log(k + squared_error)\n",
    "    return np.mean(scaled_error)\n",
    "\n",
    "\n",
    "# Calculate evaluation metrics\n",
    "def evaluate_predictions(y_true, y_pred, k=1.0):\n",
    "    \"\"\"\n",
    "    Evaluate predictions using MSE, MAE, MAPE, MRPE, and t_loss.\n",
    "\n",
    "    Args:\n",
    "        y_true (np.array): Ground truth values.\n",
    "        y_pred (np.array): Predicted values.\n",
    "        k (float): Scaling parameter for t-distribution loss.\n",
    "\n",
    "    Returns:\n",
    "        metrics (dict): Dictionary of evaluation metrics.\n",
    "    \"\"\"\n",
    "    mse = mean_squared_error(y_true, y_pred)\n",
    "    mae = mean_absolute_error(y_true, y_pred)\n",
    "    mape = np.mean(np.abs((y_true - y_pred) / y_true)) * 100  # Avoid dividing by zero\n",
    "    mrpe = np.mean(np.abs((y_true - y_pred) / y_pred)) * 100\n",
    "    t_loss_value = t_loss_metric(y_true, y_pred, k)\n",
    "\n",
    "    return {\"MSE\": mse, \"MAE\": mae, \"MAPE\": mape, \"MRPE\": mrpe, \"t_loss\": t_loss_value}\n",
    "\n",
    "\n",
    "# Ground truth (y_test) should be prepared from your test dataset\n",
    "y_test = test[y_col].values.flatten()\n",
    "\n",
    "# Evaluate predictions\n",
    "metrics = evaluate_predictions(y_test, predictions, k=k)\n",
    "\n",
    "# Print metrics\n",
    "print(\"===== Evaluation Metrics =====\")\n",
    "for metric, value in metrics.items():\n",
    "    print(f\"{metric}: {value:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MeNets\n",
    "#### MAE vs t-loss\n",
    "- MAE Loss\n",
    "    - MSE: 0.0125\n",
    "    - MAE: 0.0689\n",
    "    - MAPE: 63799.8312\n",
    "    - MRPE: 266.0444\n",
    "\n",
    "- t-loss Loss\n",
    "    - MSE: 0.0200\n",
    "    - MAE: 0.0902\n",
    "    - MAPE: 103552.1810\n",
    "    - MRPE: 216.8140\n",
    "    - t_loss: 1.9487"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "t-menet-EdH5hU3Y-py3.11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
