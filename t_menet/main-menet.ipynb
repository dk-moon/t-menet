{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import *\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Load\n",
    "data_path = 'data/t_distribution_data.csv'\n",
    "data = pd.read_csv(data_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Preprocessing\n",
    "le = LabelEncoder()\n",
    "data['id'] = le.fit_transform(data['id'])\n",
    "clusters = sorted(list(set(data['id'].values))) # unique cluster id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Torch Dataset Class\n",
    "class SIMDataset(Dataset):\n",
    "    def __init__(self, data, targets, clusters):\n",
    "        self.data = data\n",
    "        self.targets = targets\n",
    "        self.clusters = clusters\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        return self.data[index], self.targets[index], self.clusters[index]\n",
    "\n",
    "x_col = ['id', 'x'] # input columns\n",
    "y_col = ['y'] # target columns\n",
    "\n",
    "# Train-Test Split\n",
    "SEED = 42\n",
    "train, test = train_test_split(data, test_size=0.333, random_state=SEED, stratify=data['id'])\n",
    "\n",
    "# Dataset Creation\n",
    "train_dataset = SIMDataset(train[x_col].values, train[y_col].values, train['id'].values)\n",
    "test_dataset = SIMDataset(test[x_col].values, test[y_col].values, test['id'].values)\n",
    "\n",
    "batch_size = 32\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define PyTorch MeNet Model\n",
    "class MeNet(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim=[20, 5]):\n",
    "        super(MeNet, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_dim, hidden_dim[0])\n",
    "        self.fc2 = nn.Linear(hidden_dim[0], hidden_dim[1])\n",
    "        self.fc3 = nn.Linear(hidden_dim[1], 1)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.relu(self.fc1(x))\n",
    "        x = self.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model Training\n",
    "def train_menet(model, train_loader, n_clusters, device, epochs=100, lr=0.001, patience=10):\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "    criterion = nn.MSELoss()\n",
    "\n",
    "    # Initialize Random Effects\n",
    "    output_dim = model.fc2.weight.data.size(0)  # Output dimension from fc2 weights\n",
    "    b_hat = torch.zeros(n_clusters, output_dim, device=device)\n",
    "    D_hat = torch.eye(output_dim, device=device)\n",
    "    sig2e_est = 1.0\n",
    "\n",
    "    best_loss = float('inf')\n",
    "    wait = 0\n",
    "\n",
    "    for epoch in tqdm(range(epochs)):\n",
    "        model.train()\n",
    "        epoch_loss = 0\n",
    "        for batch in train_loader:\n",
    "            X_batch, y_batch, cluster_ids = batch\n",
    "            X_batch, y_batch = X_batch.to(device).float(), y_batch.to(device).float()\n",
    "\n",
    "            # Forward pass to get Z (feature map)\n",
    "            with torch.no_grad():\n",
    "                feature_map = model.fc2(model.relu(model.fc1(X_batch)))  # Shape: [batch_size, hidden_dim[1]]\n",
    "            \n",
    "            # Debugging Z shape\n",
    "            # print(f\"Feature Map shape: {feature_map.shape}, Expected shape: [batch_size, {output_dim}]\")\n",
    "\n",
    "            # E-Step: Update Random Effects\n",
    "            for cluster_id in range(n_clusters):\n",
    "                indices = (cluster_ids == cluster_id).nonzero(as_tuple=True)[0]\n",
    "                if indices.numel() == 0:\n",
    "                    continue\n",
    "\n",
    "                Z_i = feature_map[indices]  # Feature map for the current cluster\n",
    "                y_i = y_batch[indices].squeeze(-1)  # Remove extra dimension, Shape: [len(indices)]\n",
    "                f_hat_i = model(X_batch[indices]).squeeze()  # Shape: [len(indices)]\n",
    "\n",
    "                # Debugging shapes\n",
    "                # print(f\"Z_i shape: {Z_i.shape}, D_hat shape: {D_hat.shape}, y_i shape: {y_i.shape}, f_hat_i shape: {f_hat_i.shape}\")\n",
    "\n",
    "                # Ensure dimensions match for matrix multiplication\n",
    "                V_hat_i = Z_i @ D_hat @ Z_i.T + sig2e_est * torch.eye(Z_i.size(0), device=device)\n",
    "                V_hat_inv_i = torch.linalg.inv(V_hat_i)\n",
    "\n",
    "                # Expand (y_i - f_hat_i) to 2D for matrix multiplication\n",
    "                residual = (y_i - f_hat_i).unsqueeze(-1)  # Shape: [len(indices), 1]\n",
    "\n",
    "                # Update b_hat\n",
    "                b_hat[cluster_id] = (D_hat @ Z_i.T @ V_hat_inv_i @ residual).squeeze(-1)\n",
    "\n",
    "            # M-Step: Update Fixed Effects\n",
    "            optimizer.zero_grad()\n",
    "            y_pred = model(X_batch).squeeze()\n",
    "            loss = criterion(y_pred, y_batch)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            epoch_loss += loss.item()\n",
    "\n",
    "        epoch_loss /= len(train_loader)\n",
    "        print(f\"Epoch {epoch + 1}/{epochs} : Loss {epoch_loss:.4f}\")\n",
    "\n",
    "        # Early Stopping\n",
    "        if epoch_loss < best_loss:\n",
    "            best_loss = epoch_loss\n",
    "            wait = 0\n",
    "        else:\n",
    "            wait += 1\n",
    "            if wait >= patience:\n",
    "                print(\"Early stopping triggered\")\n",
    "                break\n",
    "\n",
    "    return model, b_hat, sig2e_est"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model Inference\n",
    "def menet_predict(model, test_loader, b_hat, n_clusters, device):\n",
    "    \"\"\"\n",
    "    Perform inference using the trained MeNet model with random effects.\n",
    "\n",
    "    Args:\n",
    "        model: Trained PyTorch model.\n",
    "        test_loader: DataLoader for the test dataset.\n",
    "        b_hat: Tensor containing random effect estimates for each cluster.\n",
    "        n_clusters: Number of clusters (unique IDs).\n",
    "        device: Device for computation (CPU or GPU).\n",
    "\n",
    "    Returns:\n",
    "        predictions: NumPy array of predictions for the test dataset.\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    predictions = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in test_loader:\n",
    "            X_batch, _, cluster_ids = batch\n",
    "            X_batch = X_batch.to(device).float()\n",
    "\n",
    "            # Forward pass through the model to compute fixed effects\n",
    "            y_pred = model(X_batch).squeeze()  # Shape: [batch_size]\n",
    "\n",
    "            # Apply random effects to each cluster\n",
    "            for cluster_id in range(n_clusters):\n",
    "                indices = (cluster_ids == cluster_id).nonzero(as_tuple=True)[0]\n",
    "                if indices.numel() == 0:\n",
    "                    continue\n",
    "\n",
    "                # Apply random effects adjustment for the current cluster\n",
    "                feature_map = model.fc2(model.relu(model.fc1(X_batch)))  # Shape: [batch_size, hidden_dim[1]]\n",
    "                Z_i = feature_map[indices]  # Feature map for the current cluster\n",
    "                adjustment = Z_i @ b_hat[cluster_id].unsqueeze(-1)  # Shape: [len(indices), 1]\n",
    "                y_pred[indices] += adjustment.squeeze(-1)\n",
    "\n",
    "            predictions.extend(y_pred.cpu().numpy())  # Convert predictions to NumPy\n",
    "\n",
    "    return np.array(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Main Execution\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "input_dim = len(x_col)\n",
    "n_clusters = len(clusters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model Initialization\n",
    "model = MeNet(input_dim).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/100 [00:00<?, ?it/s]/opt/anaconda3/lib/python3.12/site-packages/torch/nn/modules/loss.py:608: UserWarning: Using a target size (torch.Size([32, 1])) that is different to the input size (torch.Size([32])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/opt/anaconda3/lib/python3.12/site-packages/torch/nn/modules/loss.py:608: UserWarning: Using a target size (torch.Size([27, 1])) that is different to the input size (torch.Size([27])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "  9%|▉         | 9/100 [00:00<00:02, 38.15it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100 : Loss 0.0499\n",
      "Epoch 2/100 : Loss 0.0213\n",
      "Epoch 3/100 : Loss 0.0144\n",
      "Epoch 4/100 : Loss 0.0120\n",
      "Epoch 5/100 : Loss 0.0110\n",
      "Epoch 6/100 : Loss 0.0106\n",
      "Epoch 7/100 : Loss 0.0104\n",
      "Epoch 8/100 : Loss 0.0102\n",
      "Epoch 9/100 : Loss 0.0100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 18%|█▊        | 18/100 [00:00<00:02, 37.43it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10/100 : Loss 0.0101\n",
      "Epoch 11/100 : Loss 0.0101\n",
      "Epoch 12/100 : Loss 0.0100\n",
      "Epoch 13/100 : Loss 0.0100\n",
      "Epoch 14/100 : Loss 0.0100\n",
      "Epoch 15/100 : Loss 0.0100\n",
      "Epoch 16/100 : Loss 0.0101\n",
      "Epoch 17/100 : Loss 0.0100\n",
      "Epoch 18/100 : Loss 0.0101\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 23%|██▎       | 23/100 [00:00<00:01, 38.86it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 19/100 : Loss 0.0100\n",
      "Epoch 20/100 : Loss 0.0099\n",
      "Epoch 21/100 : Loss 0.0100\n",
      "Epoch 22/100 : Loss 0.0100\n",
      "Epoch 23/100 : Loss 0.0099\n",
      "Epoch 24/100 : Loss 0.0099\n",
      "Epoch 25/100 : Loss 0.0100\n",
      "Epoch 26/100 : Loss 0.0098\n",
      "Epoch 27/100 : Loss 0.0099\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 32%|███▏      | 32/100 [00:00<00:01, 34.24it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 28/100 : Loss 0.0099\n",
      "Epoch 29/100 : Loss 0.0099\n",
      "Epoch 30/100 : Loss 0.0099\n",
      "Epoch 31/100 : Loss 0.0099\n",
      "Epoch 32/100 : Loss 0.0102\n",
      "Epoch 33/100 : Loss 0.0098\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 35%|███▌      | 35/100 [00:00<00:01, 35.49it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 34/100 : Loss 0.0099\n",
      "Epoch 35/100 : Loss 0.0099\n",
      "Epoch 36/100 : Loss 0.0098\n",
      "Early stopping triggered\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Train the Model\n",
    "model, b_hat, sig2e_est = train_menet(model, train_loader, n_clusters, device, epochs=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predictions: [ 0.026841    0.02152333  0.01048341  0.03317363  0.02105941  0.00579274\n",
      "  0.02823902  0.01614921  0.00978274  0.03449761  0.03274583  0.03394176\n",
      "  0.02439539  0.01868584  0.00238098  0.02549402  0.0297981   0.01108495\n",
      "  0.02497786  0.06573636 -0.01682149  0.00842068  0.01425993  0.11389747\n",
      "  0.01766851 -0.04875698  0.11411142  0.01639492  0.02290808  0.05536727\n",
      "  0.01338203  0.0078006   0.01840146  0.01512287 -0.05439264  0.02384457\n",
      "  0.02457746  0.11450274  0.04425842  0.01475005  0.0115121   0.07566318\n",
      "  0.0304903   0.02709374  0.02448831  0.01368714  0.03750501  0.03213113\n",
      "  0.09931414  0.01891589  0.04129791  0.03705351  0.04270222  0.02911329\n",
      "  0.04258706  0.03589354  0.11823933  0.03963827  0.00895112  0.05186625\n",
      "  0.0960172   0.03316765  0.03824171  0.02355003  0.02866612  0.03104725\n",
      " -0.00108824  0.01728559  0.01283209  0.02488139  0.04201642  0.02719539\n",
      "  0.02186912  0.08920351  0.04724576  0.03582845  0.06578904  0.00704966\n",
      "  0.00976964  0.00932837  0.00993091  0.03198825  0.03003559 -0.04687842\n",
      "  0.01201399  0.03392571  0.01746218  0.06356768  0.02516402  0.10623776\n",
      "  0.01322319  0.022503    0.02673429  0.01190469  0.01849166  0.03577694\n",
      "  0.01196321  0.00916068 -0.00517205  0.0127112   0.0298857   0.02384097\n",
      "  0.02161586  0.00818584  0.04559999  0.07234027 -0.04499985  0.09451006\n",
      "  0.09781264  0.02847954  0.01886331  0.01677193  0.04720749  0.01817361\n",
      "  0.08040416  0.01948197 -0.00173854  0.01671859  0.02048878  0.01596163\n",
      "  0.01211665  0.01242264  0.03235462  0.07329751  0.02532984  0.01007301\n",
      "  0.02851122  0.10985412  0.10994159  0.02505547  0.0113741   0.01513528\n",
      "  0.04244988  0.02749982  0.06079028  0.0154767   0.09129158  0.00051919\n",
      "  0.11396118  0.03612413  0.01194385  0.03527753  0.03042394  0.01864774\n",
      "  0.02880244  0.03520767  0.01328484  0.09359945  0.04181371 -0.00637951\n",
      "  0.03067415  0.0120013   0.03180955  0.02035179  0.00394022  0.11961944\n",
      "  0.06886487  0.02784602  0.03315867  0.01295568 -0.00022943  0.04769458\n",
      "  0.02693389  0.01492403  0.02916538  0.02542196  0.0096677   0.08997281\n",
      "  0.02480227  0.03615722  0.10250121  0.029686    0.06777908 -0.0065012\n",
      "  0.03989713  0.06602816  0.01743609  0.06626537  0.01784554  0.03408132\n",
      "  0.01097183  0.01830572  0.11412507  0.02707366  0.01812097  0.02524028\n",
      "  0.01625213  0.02904084  0.02153875  0.02635486  0.01964704  0.01406857\n",
      "  0.09854485  0.01332354  0.00934736  0.05895417  0.09278785  0.00634575\n",
      "  0.01805857  0.02242158  0.06470382  0.02171662  0.01945238  0.08777482\n",
      "  0.06893586  0.01358658  0.02558644  0.03453209  0.00817611  0.03303757\n",
      "  0.11944693  0.01201005  0.02823585  0.01506076  0.00758983  0.02463161\n",
      "  0.06690364 -0.00789835  0.03128217  0.1146939   0.0145655   0.02493884\n",
      "  0.05725406  0.00840308  0.1211501   0.02882472  0.05310274  0.0107528\n",
      "  0.01696797  0.01029371  0.01632979  0.02087412  0.01877789  0.08041165\n",
      "  0.02817598  0.06195237  0.00133506  0.04288777  0.03048555  0.02992861\n",
      "  0.114926    0.02314654  0.04455945  0.1196022   0.01359042  0.01552961\n",
      "  0.03606231  0.06185746  0.02109018  0.07626754  0.0932249   0.02867974\n",
      "  0.01799622  0.03257229 -0.03466778  0.0646998   0.09788547  0.00373089\n",
      "  0.0106338   0.02710525  0.02204541  0.01003354  0.04877133  0.08775924\n",
      "  0.01544274 -0.00224157  0.02827777  0.02246147  0.02526531  0.02693532\n",
      "  0.04261424  0.0232936   0.02775545  0.00706445  0.03870125  0.11029146\n",
      "  0.02120412  0.01520472  0.11488961  0.06867756  0.01787817  0.02596692\n",
      "  0.07537883  0.04747017  0.07760528  0.05574278  0.0344794   0.01923939\n",
      "  0.03911771  0.01567268  0.00836019  0.01705945  0.02256799 -0.00631261\n",
      "  0.02230098  0.12327757  0.01796753  0.02458794  0.03109813  0.02625134\n",
      "  0.11437081  0.0092257   0.02499893 -0.00257694  0.03342991  0.01478911\n",
      "  0.03108276  0.03159886  0.02927932  0.01572096  0.03925652  0.03115689\n",
      "  0.09756234  0.03767627  0.02983119  0.06194293  0.01303494  0.03918711\n",
      "  0.03782406  0.03666412  0.03804784  0.01586913 -0.00364061  0.01458206\n",
      "  0.07350896  0.02369808  0.03073492  0.06937089  0.02828548  0.09467068\n",
      "  0.07276218  0.03881002  0.02457108]\n"
     ]
    }
   ],
   "source": [
    "# Predict\n",
    "predictions = menet_predict(\n",
    "    model=model,\n",
    "    test_loader=test_loader,\n",
    "    b_hat=b_hat,\n",
    "    n_clusters=n_clusters,\n",
    "    device=device,\n",
    ")\n",
    "\n",
    "# Print predictions\n",
    "print(\"Predictions:\", predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===== Evaluation Metrics =====\n",
      "MSE: 0.0125\n",
      "MAE: 0.0689\n",
      "MAPE: 63799.8312\n",
      "MRPE: 266.0444\n"
     ]
    }
   ],
   "source": [
    "# Calculate evaluation metrics\n",
    "def evaluate_predictions(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    Evaluate predictions using MSE, MAE, MAPE, and MRPE.\n",
    "    \n",
    "    Args:\n",
    "        y_true (np.array): Ground truth values.\n",
    "        y_pred (np.array): Predicted values.\n",
    "    \n",
    "    Returns:\n",
    "        metrics (dict): Dictionary of evaluation metrics.\n",
    "    \"\"\"\n",
    "    mse = mean_squared_error(y_true, y_pred)\n",
    "    mae = mean_absolute_error(y_true, y_pred)\n",
    "    mape = np.mean(np.abs((y_true - y_pred) / y_true)) * 100  # Avoid dividing by zero\n",
    "    mrpe = np.mean(np.abs((y_true - y_pred) / y_pred)) * 100\n",
    "\n",
    "    return {\n",
    "        \"MSE\": mse,\n",
    "        \"MAE\": mae,\n",
    "        \"MAPE\": mape,\n",
    "        \"MRPE\": mrpe\n",
    "    }\n",
    "\n",
    "# Ground truth (y_test) should be prepared from your test dataset\n",
    "y_test = test[y_col].values.flatten()\n",
    "\n",
    "# Evaluate predictions\n",
    "metrics = evaluate_predictions(y_test, predictions)\n",
    "\n",
    "# Print metrics\n",
    "print(\"===== Evaluation Metrics =====\")\n",
    "for metric, value in metrics.items():\n",
    "    print(f\"{metric}: {value:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
